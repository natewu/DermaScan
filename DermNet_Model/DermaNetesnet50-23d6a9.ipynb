{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))","execution_count":5,"outputs":[{"output_type":"stream","text":"['skin-cancer-mnist-ham10000']\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \n\ndf = pd.read_csv('../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\ndf.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os.path import isfile\nfrom PIL import Image as pil_image\ndf['num_images'] = df.groupby('lesion_id')[\"image_id\"].transform(\"count\")\n\nclasses = df['dx'].unique()\nlabeldict = {}\nfor num, name in enumerate(classes):\n    labeldict[name] = num\ndf['dx_id'] = df['dx'].map(lambda x: labeldict[x])\n\n\ndef expand_path(p):\n    if isfile('../input/skin-cancer-mnist-ham10000/ham10000_images_part_1/' + p + '.jpg'): return '../input/skin-cancer-mnist-ham10000/ham10000_images_part_1/' + p + '.jpg'\n    if isfile('../input/skin-cancer-mnist-ham10000/ham10000_images_part_2/' + p + '.jpg'): return '../input/skin-cancer-mnist-ham10000/ham10000_images_part_2/' + p + '.jpg'\n    return p \ndf['image_path'] = df['image_id']\ndf['image_path'] = df['image_path'].apply(expand_path)\n\n\ndf['images'] = df['image_path'].map(lambda x: np.asarray(pil_image.open(x).resize((150,112))))\ndf.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization  \\\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n\n   num_images  dx_id                                         image_path  \\\n0           2      0  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n1           2      0  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n2           2      0  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n3           2      0  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n4           2      0  ../input/skin-cancer-mnist-ham10000/ham10000_i...   \n\n                                              images  \n0  [[[188, 151, 193], [193, 156, 198], [192, 155,...  \n1  [[[24, 13, 22], [24, 13, 22], [24, 14, 25], [2...  \n2  [[[186, 126, 135], [189, 131, 142], [192, 136,...  \n3  [[[23, 11, 16], [24, 11, 19], [26, 13, 22], [3...  \n4  [[[129, 87, 109], [139, 94, 117], [148, 102, 1...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>num_images</th>\n      <th>dx_id</th>\n      <th>image_path</th>\n      <th>images</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>2</td>\n      <td>0</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>[[[188, 151, 193], [193, 156, 198], [192, 155,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>2</td>\n      <td>0</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>[[[24, 13, 22], [24, 13, 22], [24, 14, 25], [2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>2</td>\n      <td>0</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>[[[186, 126, 135], [189, 131, 142], [192, 136,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>2</td>\n      <td>0</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>[[[23, 11, 16], [24, 11, 19], [26, 13, 22], [3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>2</td>\n      <td>0</td>\n      <td>../input/skin-cancer-mnist-ham10000/ham10000_i...</td>\n      <td>[[[129, 87, 109], [139, 94, 117], [148, 102, 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_single = df[df['num_images'] == 1]\ntrainset1, testset = train_test_split(df_single, test_size=0.2,random_state = 80)\ntrainset2, validationset = train_test_split(trainset1, test_size=0.2,random_state = 600)\ntrainset3 = df[df['num_images'] != 1]\nframes = [trainset2, trainset3]\ntrainset = pd.concat(frames)\ndef prepareimages(images):\n    # images is a list of images\n    images = np.asarray(images).astype(np.float64)\n    images = images[:, :, :, ::-1]\n    m0 = np.mean(images[:, :, :, 0])\n    m1 = np.mean(images[:, :, :, 1])\n    m2 = np.mean(images[:, :, :, 2])\n    images[:, :, :, 0] -= m0\n    images[:, :, :, 1] -= m1\n    images[:, :, :, 2] -= m2\n    return images\ntrainimages = prepareimages(list(trainset['images']))\ntestimages = prepareimages(list(testset['images']))\nvalidationimages = prepareimages(list(validationset['images']))\ntrainlabels = np.asarray(trainset['dx_id'])\ntestlabels = np.asarray(testset['dx_id'])\nvalidationlabels = np.asarray(validationset['dx_id'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train image shape: \",trainimages.shape)\nprint(\"Test image shape: \",testimages.shape)\nprint(\"Validation image shape: \", validationimages.shape)\nprint(\"Train label shape: \",trainlabels.shape)\nprint(\"validation label shape: \",validationlabels.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Train image shape:  (8029, 112, 150, 3)\nTest image shape:  (1103, 112, 150, 3)\nValidation image shape:  (883, 112, 150, 3)\nTrain label shape:  (8029,)\nvalidation label shape:  (883,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.hist(trainlabels,bins = 7,density = True)\nplt.show()\n\nplt.hist(validationlabels,bins = 7,density= True)\nplt.show()\n\nplt.hist(testlabels,bins = 7,density= True)\nplt.show()","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrainimages = trainimages.reshape(trainimages.shape[0], *(112, 150, 3))\n\nimport tensorflow as tf\n\nHEIGHT = 112\nWIDTH = 150\n\ndef data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n#     p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    \n#     if p_cutout > .5:\n#         image = data_augment_cutout(image)\n        \n    \n    return image\n#This cutouts the images, and ML has to predict more on it(aggressive augmentation)\ndef data_augment_cutout(image, min_mask_size=(float(HEIGHT *.1), float(HEIGHT *.1)),\n                       max_mask_size=(float(HEIGHT * .125), float(HEIGHT * .125))):\n    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    if p_cutout > .85: # 10~15 cut outs\n        n_cutout = tf.random.uniform([], 10, 15, dtype=tf.float32)\n        image = random_cutout(image, HEIGHT, WIDTH, \n                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n    elif p_cutout > .6: # 5~10 cut outs\n        n_cutout = tf.random.uniform([], 5, 10, dtype=tf.float32)\n        image = random_cutout(image, HEIGHT, WIDTH, \n                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n    elif p_cutout > .25: # 2~5 cut outs\n        n_cutout = tf.random.uniform([], 2, 5, dtype=tf.float32)\n        image = random_cutout(image, HEIGHT, WIDTH, \n                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n    else: # 1 cut out\n        image = random_cutout(image, HEIGHT, WIDTH, \n                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=1)\n\n    return image\n#This cutouts the above function randomly, just cuts off squares of the images\ndef random_cutout(image, height, width, channels=3, min_mask_size=(10, 10), max_mask_size=(80, 80), k=1):\n    assert height > min_mask_size[0]\n    assert width > min_mask_size[1]\n    assert height > max_mask_size[0]\n    assert width > max_mask_size[1]\n\n    for i in range(k):\n      mask_height = tf.random.uniform(shape=[], minval=min_mask_size[0], maxval=max_mask_size[0], dtype=tf.float32)\n      mask_width = tf.random.uniform(shape=[], minval=min_mask_size[1], maxval=max_mask_size[1], dtype=tf.float32)\n\n      pad_h = height - mask_height\n      pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.float32)\n      pad_bottom = pad_h - pad_top\n\n      pad_w = width - mask_width\n      pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.float32)\n      pad_right = pad_w - pad_left\n\n      cutout_area = tf.zeros(shape=[mask_height, mask_width, channels], dtype=tf.uint8)\n\n      cutout_mask = tf.pad([cutout_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n      cutout_mask = tf.squeeze(cutout_mask, axis=0)\n      image = tf.multiply(tf.cast(image, tf.float32), tf.cast(cutout_mask, tf.float32))\n\n    return image\n\n#This is experimental, this will freeze certain layers to make model train better  \ndef unfreeze_model(model):\n    # Unfreeze layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, L.BatchNormalization):\n            layer.trainable = True\n        else:\n            layer.trainable = False\n            \ndef unfreeze_block(model, block_name=None, n_top=3):\n    # Unfreeze layers while leaving BatchNorm layers frozen\n    for layer in model.layers[:-n_top]:\n        if isinstance(layer, L.BatchNormalization):\n            layer.trainable = False\n        else:\n            if block_name and (block_name in layer.name):\n                layer.trainable = True\n    \n#Here is where I create the 3 functions (get_label) to (make_dataset_unbactched) to turn the image filenames into tensorflow BatchedDataset\n\ndef parse_image(image, augment = False):\n    image = tf.cast(image, dtype=tf.float32)\n    if augment:\n        image = data_augment(image)\n    image = tf.image.resize(image, [112, 150])/255.0 # size the image and normalize\n    \n    return image\n\ntrainimages = parse_image(trainimages, augment=True)","execution_count":11,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'numpy.float32' object has no attribute '__index__'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-61932cf01ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mtrainimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-61932cf01ba2>\u001b[0m in \u001b[0;36mparse_image\u001b[0;34m(image, augment)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m \u001b[0;31m# size the image and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-61932cf01ba2>\u001b[0m in \u001b[0;36mdata_augment\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp_cutout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augment_cutout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-61932cf01ba2>\u001b[0m in \u001b[0;36mdata_augment_cutout\u001b[0;34m(image, min_mask_size, max_mask_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mn_cutout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         image = random_cutout(image, HEIGHT, WIDTH, \n\u001b[0;32m---> 47\u001b[0;31m                               min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 1 cut out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         image = random_cutout(image, HEIGHT, WIDTH, \n","\u001b[0;32m<ipython-input-11-61932cf01ba2>\u001b[0m in \u001b[0;36mrandom_cutout\u001b[0;34m(image, height, width, channels, min_mask_size, max_mask_size, k)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_mask_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mmask_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_mask_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_mask_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mmask_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_mask_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_mask_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute '__index__'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train image shape: \",trainimages.shape)\nprint(\"Test image shape: \",testimages.shape)\nprint(\"Validation image shape: \", validationimages.shape)\nprint(\"Train label shape: \",trainlabels.shape)\nprint(\"validation label shape: \",validationlabels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import optimizers\nfrom keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nfrom keras.applications import ResNet50\nfrom keras import regularizers\nimport numpy as np\n\nmodel = Sequential()\nnum_labels = 7\n\nbase_model = ResNet50(include_top=False,input_shape=(224,224,3),pooling = 'avg', weights=\"imagenet\")\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation=\"relu\",kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_labels, activation = 'softmax',kernel_regularizer=regularizers.l2(0.02)))\n\nfor layer in base_model.layers:\n    layer.trainable = False\nfor layer in base_model.layers[-22:]:\n    layer.trainable = True\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train image shape: \",trainimages.shape)\nprint(\"Test image shape: \",testimages.shape)\nprint(\"Validation image shape: \", validationimages.shape)\nprint(\"Train label shape: \",trainlabels.shape)\nprint(\"validation label shape: \",validationlabels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\noptimizer = Adam (lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train image shape: \",trainimages.shape)\nprint(\"Test image shape: \",testimages.shape)\nprint(\"Validation image shape: \", validationimages.shape)\nprint(\"Train label shape: \",trainlabels.shape)\nprint(\"validation label shape: \",validationlabels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nimport keras\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n# class CustomModelCheckPoint(keras.callbacks.Callback):\n#     def __init__(self,**kargs):\n#         super(CustomModelCheckPoint,self).__init__(**kargs)\n#         self.epoch_accuracy = {} # loss at given epoch\n#         self.epoch_loss = {} # accuracy at given epoch\n#     def on_epoch_begin(self,epoch, logs={}):\n#             # Things done on beginning of epoch. \n#             return\n\n#     def on_epoch_end(self, epoch, logs={}):\n#             # things done on end of the epoch\n#             self.epoch_accuracy[epoch] = logs.get(\"acc\")\n#             self.epoch_loss[epoch] = logs.get(\"loss\")\n#             self.model.save_weights(\"../output/resnet50/name-of-model-%d.h5\" %epoch)\n            \ncb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = 4)\ncb_checkpointer = ModelCheckpoint(filepath = '../working/best.h5', monitor = 'val_loss', save_best_only = True,save_weights_only = False, mode = 'auto')\n            \nepochs = 30 \nbatch_size = 20\ntrainhistory = model.fit(trainimages,trainlabels, \n                        batch_size=batch_size,\n                        epochs = epochs, \n                        validation_data = (validationimages,validationlabels),\n                        verbose = 1, steps_per_epoch=trainimages.shape[0] // batch_size,\n                        callbacks=[cb_checkpointer, cb_early_stopper])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train image shape: \",trainimages.shape)\nprint(\"Test image shape: \",testimages.shape)\nprint(\"Validation image shape: \", validationimages.shape)\nprint(\"Train label shape: \",trainlabels.shape)\nprint(\"validation label shape: \",validationlabels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = trainhistory.history['accuracy']\nval_acc = trainhistory.history['val_accuracy']\nloss = trainhistory.history['loss']\nval_loss = trainhistory.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss, '', label='Training loss')\nplt.plot(epochs, val_loss, '', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc,'', label='Training accuracy')\nplt.plot(epochs, val_acc, '', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"../working/best.h5\")\ntest_loss, test_acc = model.evaluate(testimages, testlabels, verbose=1)\nprint(\"test_accuracy = %f  ;  test_loss = %f\" % (test_acc, test_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ntrain_pred = model.predict(trainimages)\ntrain_pred_classes = np.argmax(train_pred,axis = 1)\ntest_pred = model.predict(testimages)\n# Convert predictions classes to one hot vectors \ntest_pred_classes = np.argmax(test_pred,axis = 1) \n\nconfusionmatrix = confusion_matrix(testlabels, test_pred_classes)\nconfusionmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nlabels = labeldict.keys()\n# Generate a classification report\ntrainreport = classification_report(trainlabels, train_pred_classes, target_names=list(labels))\ntestreport = classification_report(testlabels, test_pred_classes, target_names=list(labels))\n\nprint(trainreport)\nprint(testreport)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}